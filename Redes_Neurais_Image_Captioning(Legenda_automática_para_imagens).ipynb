{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cYzAyr0STBFg"
      },
      "outputs": [],
      "source": [
        "import json, re, torch, numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "import torch, torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "import argparse, os, pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "PAD, BOS, EOS, UNK = \"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP4HikqUT1pf"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7_14rcEqT4tZ"
      },
      "outputs": [],
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self, min_freq=3):\n",
        "        self.min_freq = min_freq\n",
        "        self.word2id = {PAD:0, BOS:1, EOS:2, UNK:3}\n",
        "        self.id2word = {0:PAD, 1:BOS, 2:EOS, 3:UNK}\n",
        "\n",
        "    def build(self, texts):\n",
        "        tok = lambda s: nltk.word_tokenize(re.sub(r\"[^A-Za-z0-9' ]\",\" \", s.lower()))\n",
        "        cnt = Counter()\n",
        "        for t in texts:\n",
        "            cnt.update(tok(t))\n",
        "        for w, c in cnt.items():\n",
        "            if c >= self.min_freq and w not in self.word2id:\n",
        "                idx = len(self.word2id)\n",
        "                self.word2id[w] = idx\n",
        "                self.id2word[idx] = w\n",
        "\n",
        "    def encode(self, text, max_len=20):\n",
        "        tok = nltk.word_tokenize(re.sub(r\"[^A-Za-z0-9' ]\",\" \", text.lower()))\n",
        "        ids = [self.word2id.get(w, self.word2id[UNK]) for w in tok][:max_len]\n",
        "        return [self.word2id[BOS]] + ids + [self.word2id[EOS]]\n",
        "\n",
        "    def decode(self, ids):\n",
        "        words = []\n",
        "        for i in ids:\n",
        "            w = self.id2word.get(int(i), UNK)\n",
        "            if w in [PAD, BOS]:\n",
        "                continue\n",
        "            if w == EOS:\n",
        "                break\n",
        "            words.append(w)\n",
        "        return \" \".join(words)\n",
        "\n",
        "    def to_json(self, path):\n",
        "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump({\"min_freq\": self.min_freq, \"word2id\": self.word2id}, f)\n",
        "\n",
        "    @classmethod\n",
        "    def from_json(cls, path):\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            obj = json.load(f)\n",
        "        v = cls(min_freq=obj[\"min_freq\"])\n",
        "        v.word2id = obj[\"word2id\"]\n",
        "        v.id2word = {int(i):w for w,i in [(i, w) for w,i in v.word2id.items()]}\n",
        "        return v\n",
        "\n",
        "class CaptionDataset(Dataset):\n",
        "    def __init__(self, df, images_root, vocab, split=\"train\", max_len=20, image_size=224):\n",
        "        self.df = df[df[\"split\"] == split].reset_index(drop=True)\n",
        "        self.images_root = images_root\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "        self.tf = transforms.Compose([\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = f\"{self.images_root}/{row['image_path']}\"\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = self.tf(img)\n",
        "        ids = self.vocab.encode(row[\"caption\"], max_len=self.max_len)\n",
        "        return img, torch.tensor(ids, dtype=torch.long)\n",
        "\n",
        "def pad_collate(batch):\n",
        "    imgs, seqs = zip(*batch)\n",
        "    imgs = torch.stack(imgs, dim=0)\n",
        "    lengths = [len(s) for s in seqs]\n",
        "    maxlen = max(lengths)\n",
        "    PAD_ID = 0\n",
        "    padded = torch.full((len(seqs), maxlen), PAD_ID, dtype=torch.long)\n",
        "    for i, s in enumerate(seqs):\n",
        "        padded[i, :len(s)] = s\n",
        "    return imgs, padded, torch.tensor(lengths, dtype=torch.long)\n",
        "\n",
        "def compute_bleu(gens, refs, n=4):\n",
        "    import nltk\n",
        "    weights_map = {\n",
        "        1:(1.0, 0, 0, 0),\n",
        "        2:(0.5, 0.5, 0, 0),\n",
        "        3:(1/3, 1/3, 1/3, 0),\n",
        "        4:(0.25, 0.25, 0.25, 0.25)\n",
        "    }\n",
        "    weights = weights_map.get(n, weights_map[4])\n",
        "    refs_tok = [[nltk.word_tokenize(r.lower())] for r in refs]\n",
        "    gens_tok = [nltk.word_tokenize(g.lower()) for g in gens]\n",
        "    try:\n",
        "        return nltk.translate.bleu_score.corpus_bleu(refs_tok, gens_tok, weights=weights)\n",
        "    except ZeroDivisionError:\n",
        "        return 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gzyDOpJUKbn"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Vmb6kc2kUPZi"
      },
      "outputs": [],
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_dim=256):\n",
        "        super().__init__()\n",
        "        backbone = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "        for p in backbone.parameters():\n",
        "            p.requires_grad = False\n",
        "        modules = list(backbone.children())[:-1]\n",
        "        self.cnn = nn.Sequential(*modules)\n",
        "        self.fc = nn.Linear(backbone.fc.in_features, embed_dim)\n",
        "        self.bn = nn.BatchNorm1d(embed_dim, momentum=0.01)\n",
        "\n",
        "    def forward(self, images):\n",
        "        with torch.no_grad():\n",
        "            feats = self.cnn(images).squeeze()\n",
        "            if feats.dim() == 1:\n",
        "                feats = feats.unsqueeze(0)\n",
        "        feats = self.fc(feats)\n",
        "        feats = self.bn(feats)\n",
        "        return torch.relu(feats)\n",
        "\n",
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512, num_layers=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        emb = self.embed(captions)\n",
        "        feats = features.unsqueeze(1)\n",
        "        x = torch.cat([feats, emb], dim=1)\n",
        "        out, _ = self.lstm(x)\n",
        "        logits = self.fc(out)\n",
        "        return logits\n",
        "\n",
        "    def sample(self, features, max_len=20, bos_id=1, eos_id=2):\n",
        "        B = features.size(0)\n",
        "        inputs = features.unsqueeze(1)\n",
        "        states = None\n",
        "        outputs = []\n",
        "        for _ in range(max_len):\n",
        "            out, states = self.lstm(inputs, states)\n",
        "            logits = self.fc(out[:, -1, :])\n",
        "            _, next_ids = torch.max(logits, dim=1)\n",
        "            outputs.append(next_ids)\n",
        "            emb = self.embed(next_ids).unsqueeze(1)\n",
        "            inputs = emb\n",
        "            if (next_ids == eos_id).all():\n",
        "                break\n",
        "        return torch.stack(outputs, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6dZjjEBUnXz"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApgOhkTHUw3c"
      },
      "outputs": [],
      "source": [
        "def plot_curves(history, outpath):\n",
        "    fig, ax = plt.subplots(figsize=(7,5))\n",
        "    ax.plot(history[\"train_loss\"], label=\"train_loss\")\n",
        "    ax.plot(history[\"val_loss\"], label=\"val_loss\")\n",
        "    ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Loss (CE)\"); ax.set_title(\"Training & Validation Loss\")\n",
        "    ax.legend(); fig.tight_layout(); fig.savefig(outpath, dpi=160); plt.close(fig)\n",
        "\n",
        "def plot_bleu(bleus, outpath):\n",
        "    fig, ax = plt.subplots(figsize=(7,5))\n",
        "    ax.plot(bleus, label=\"BLEU-4\")\n",
        "    ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"BLEU\"); ax.set_title(\"Validation BLEU-4\")\n",
        "    ax.legend(); fig.tight_layout(); fig.savefig(outpath, dpi=160); plt.close(fig)\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--captions\", required=True, help=\"CSV with columns: image_path, caption, split\")\n",
        "    ap.add_argument(\"--images-root\", type=str, default=\"data\")\n",
        "    ap.add_argument(\"--outdir\", type=str, default=\"outputs\")\n",
        "    ap.add_argument(\"--epochs\", type=int, default=10)\n",
        "    ap.add_argument(\"--batch-size\", type=int, default=64)\n",
        "    ap.add_argument(\"--embed-dim\", type=int, default=256)\n",
        "    ap.add_argument(\"--hidden-dim\", type=int, default=512)\n",
        "    ap.add_argument(\"--min-freq\", type=int, default=3)\n",
        "    ap.add_argument(\"--max-len\", type=int, default=20)\n",
        "    ap.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    ap.add_argument(\"--seed\", type=int, default=42)\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    os.makedirs(args.outdir, exist_ok=True)\n",
        "    torch.manual_seed(args.seed); np.random.seed(args.seed)\n",
        "\n",
        "    df = pd.read_csv(args.captions)\n",
        "    vocab = Vocabulary(min_freq=args.min_freq)\n",
        "    vocab.build(df[df[\"split\"]==\"train\"][\"caption\"].tolist())\n",
        "    vocab.to_json(os.path.join(args.outdir, \"vocab.json\"))\n",
        "\n",
        "    train_ds = CaptionDataset(df, args.images_root, vocab, split=\"train\", max_len=args.max_len)\n",
        "    val_ds = CaptionDataset(df, args.images_root, vocab, split=\"val\", max_len=args.max_len)\n",
        "    train_dl = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, collate_fn=pad_collate, num_workers=2)\n",
        "    val_dl = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, collate_fn=pad_collate, num_workers=2)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    enc = EncoderCNN(embed_dim=args.embed_dim).to(device)\n",
        "    dec = DecoderLSTM(vocab_size=len(vocab.word2id), embed_dim=args.embed_dim, hidden_dim=args.hidden_dim).to(device)\n",
        "\n",
        "    crit = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    params = list(dec.parameters()) + list(enc.fc.parameters()) + list(enc.bn.parameters())\n",
        "    opt = torch.optim.Adam(params, lr=args.lr)\n",
        "\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"bleu4\": []}\n",
        "    best_bleu = -1.0\n",
        "    for ep in range(1, args.epochs+1):\n",
        "        enc.train(); dec.train(); tr_loss = 0.0; n=0\n",
        "        for imgs, tgt, lengths in tqdm(train_dl, desc=f\"Epoch {ep}/{args.epochs} [train]\"):\n",
        "            imgs, tgt = imgs.to(device), tgt.to(device)\n",
        "            opt.zero_grad()\n",
        "            feats = enc(imgs)\n",
        "            logits = dec(feats, tgt[:, :-1])\n",
        "            loss = crit(logits.reshape(-1, logits.size(-1)), tgt.reshape(-1))\n",
        "            loss.backward(); opt.step()\n",
        "            tr_loss += loss.item() * imgs.size(0); n += imgs.size(0)\n",
        "        tr = tr_loss / n\n",
        "\n",
        "        enc.eval(); dec.eval(); va_loss=0.0; vn=0\n",
        "        gens, refs = [], []\n",
        "        with torch.no_grad():\n",
        "            for imgs, tgt, lengths in tqdm(val_dl, desc=f\"Epoch {ep}/{args.epochs} [val]\"):\n",
        "                imgs, tgt = imgs.to(device), tgt.to(device)\n",
        "                feats = enc(imgs)\n",
        "                logits = dec(feats, tgt[:, :-1])\n",
        "                loss = crit(logits.reshape(-1, logits.size(-1)), tgt.reshape(-1))\n",
        "                va_loss += loss.item() * imgs.size(0); vn += imgs.size(0)\n",
        "                out_ids = dec.sample(feats, max_len=args.max_len)\n",
        "                for i in range(out_ids.size(0)):\n",
        "                    gens.append(vocab.decode(out_ids[i].cpu().numpy()))\n",
        "                    refs.append(vocab.decode(tgt[i, 1:].cpu().numpy()))\n",
        "        vl = va_loss / vn\n",
        "        bleu4 = compute_bleu(gens, refs, n=4)\n",
        "\n",
        "        history[\"train_loss\"].append(tr); history[\"val_loss\"].append(vl); history[\"bleu4\"].append(bleu4)\n",
        "        print(f\"[epoch {ep}] train_loss={tr:.4f} val_loss={vl:.4f} bleu4={bleu4:.4f}\")\n",
        "        if bleu4 > best_bleu:\n",
        "            best_bleu = bleu4\n",
        "            torch.save({\"encoder\": enc.state_dict(), \"decoder\": dec.state_dict(),\n",
        "                        \"vocab_size\": len(vocab.word2id), \"embed_dim\": args.embed_dim,\n",
        "                        \"hidden_dim\": args.hidden_dim, \"max_len\": args.max_len},\n",
        "                       os.path.join(args.outdir, \"best_captioner.pt\"))\n",
        "\n",
        "        plot_curves(history, os.path.join(args.outdir, \"training_curves.png\"))\n",
        "        plot_bleu(history[\"bleu4\"], os.path.join(args.outdir, \"bleu_scores.png\"))\n",
        "\n",
        "    with open(os.path.join(args.outdir, \"metrics.json\"), \"w\") as f:\n",
        "        json.dump({\"best_bleu4\": best_bleu}, f, indent=2)\n",
        "    print(\"[OK] Training done. Best BLEU-4:\", best_bleu)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILy-6lwJVDIU"
      },
      "source": [
        "#Infer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnLoRe-TVSGt"
      },
      "outputs": [],
      "source": [
        "def load_vocab(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        obj = json.load(f)\n",
        "    v = Vocabulary(min_freq=obj.get(\"min_freq\", 3))\n",
        "    v.word2id = obj[\"word2id\"]\n",
        "    v.id2word = {int(i): w for w, i in v.word2id.items()}\n",
        "    return v\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--checkpoint\", required=True)\n",
        "    ap.add_argument(\"--vocab\", required=True)\n",
        "    ap.add_argument(\"--image\", required=True)\n",
        "    ap.add_argument(\"--max-len\", type=int, default=20)\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    ckpt = torch.load(args.checkpoint, map_location=device)\n",
        "    vocab = load_vocab(args.vocab)\n",
        "\n",
        "    enc = EncoderCNN(embed_dim=ckpt[\"embed_dim\"]).to(device)\n",
        "    dec = DecoderLSTM(vocab_size=ckpt[\"vocab_size\"], embed_dim=ckpt[\"embed_dim\"], hidden_dim=ckpt[\"hidden_dim\"]).to(device)\n",
        "\n",
        "    enc.load_state_dict(ckpt[\"encoder\"]); dec.load_state_dict(ckpt[\"decoder\"])\n",
        "    enc.eval(); dec.eval()\n",
        "\n",
        "    tf = transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "    ])\n",
        "    img = tf(Image.open(args.image).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        feats = enc(img)\n",
        "        ids = dec.sample(feats, max_len=args.max_len, bos_id=vocab.word2id[\"<bos>\"], eos_id=vocab.word2id[\"<eos>\"])\n",
        "    caption = vocab.decode(ids[0].cpu().numpy())\n",
        "    print(caption)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
